* String collation

** Summary

	We are going to implement Windows-like collation, apart from ICU (that
	implements UCA, Unicode Collation Algorithm).


** Tasks

	* create collation element table(s)
		- infer how Windows collation element table is composed
		- infer how Windows sortkey is structured
		- design data structures
		- write table generator source(s)
	* create culture-specific sortkey modifiers
		- data structures
		- data
	* implement collation methods
		- sortkey collector methods
		- character iterator models and comparison methods
		- string search optimizations

	TODO: add "progress" information on these list items; currently many
	of them are half-baked.


** How to implement CompareInfo members

	GetSortKey()
		Compute sort key for every characters into byte[].
		Use collation element table, but Windows specific one.
	Compare()
		Find first difference and compare it. "Larger/smaller" matters.
	IsPrefix()
		For each character in the target, examine if it matches from 
		the head of the searchee and immediately return false if it 
		didn't match.
	IsSuffix()
		For each character in the target, examine if it matches
		form the tail of the searchee and immediately return false if
		it didn't match. It uses reversal element query.
	IndexOf()
		Try IsPrefix() to the end of the string to find.
	LastIndexOf()
		Try IsSuffix() from the end of the string to find.
		It uses reversal element query.

	For IndexOf() and LastIndexOf(), overloads of char argument and string 
	argument will be unified by providng common CharacterIterator base.

	For GetSortKey(), Compare(), IsPrefix() and IndexOf(), it uses forward
	iteration, which moves forward and don't stop until either it finds
	next "primary" character or it reached the end of the string, checking
	HasContractHead(char) for composite.

	For IsSuffix() and LastIndexOf(), it uses backward iteration, which
	moves backward and don't stop until either it finds "primary"
	characters or it reached the beginning of the string, checking
	HasContractTail(char) for composite.

** How to support CompareOptions

	There are two kind of "ignorance" : ignorance which acts as stripper,
	and ignorance acts as normalizer.

	The strippers will "filter characters out" and there will be no
	corresponding character elements in SortKey binaries.

	Normalizers, on the other hand, will result in certain characters 
	that is still in effect between irrelevant character and itself.
	For example, with IgnoreKanaType Hiragana "A" and Katakana "A" are
	not distinguished, but Hiragana "A" and Hiragana "I" are.

	Actually, even without any IgnoreXXX flags (i.e. "None"), there are 
	many characters that are ignored ("completely ignorable").

	For LCID 101/1125(div), '\ufdf2' is completely ignorable.
	This rule even applies to CompareOptions.None.

*** Normalizers

	IgnoreCase
		Maybe culture-dependent TextInfo.ToLower() could be used.

		Unlike ICU (specialCaseToLower()), even with tr-TR(LCID 31)
		and IgnoreCase, I\u0307 is not regarded as equal to i.

	IgnoreKanaType
		ToKanaTypeInsensitive(). Note that this does not cover the
		whole "level 4" differences described later.

	IgnoreWidth
		ToWidthInsensitive(), which is likely to be culture
		independent. See also "Notes".

	IgnoreNonSpace (see also Strippers; this flag works in both sides)
		For some cultures this logic is still incomplete. All culture-
		dependent collators must handle valid "replacement" of "one or
		more characters" which might be related to specific
		CompareOptions.
		For example, there is a Japanese text sorting rule that
		however applies to InvariantCulture. Concretely to say,
		"\u3042\u30FC" is equivalent to "\u3042\u3042" only when
		IgnoreNonSpace is specified.

		I'll take those items from CLDR (those items which has
		<reset before="..." />), case by case though.

*** Strippers

	I already wrote all the required strippers which should be MS
	compatible (at least with .NET 1.1 invariant culture).

	IgnoreNonSpace
		IsIgnorableNonSpacing().
		Some Diacritic characters are covered by this flag.

		There are some culture *dependent* characters:
			LCID 90/1114(syr) : 64b, 652, 670

	IgnoreSymbols
		IsIgnorableSymbol().
		UnicodeCategory does not work here.

		There are some culture *dependent* characters:
			LCID 17/1041(ja) : 2015
			LCID 90/1114(syr) : 64b, 652

*** StringSort

	See "sort order categories" section.

** ICU and UCA

	First to note: we won't use collation element table from unicode.org.

	To understand why we don't use collation element table from UCA, you
	can try to compare "A" and "a" in the invariant culture. Also try some
	characters like C0 that are already combined. They are COMPLETELY
	different form UCA default element table.

	So, the data is completely different, but how about resolution rules?

	Since UCA Level 3 handles both casing and width, it is impossible to
	use UCA variables for IgnoreWidth, at least with the default element
	table. And IgnoreKanaType cannot be handled without case and width
	insensitivity.

	IgnoreWidth/IgnoreSymbols is processed after Kana voice mark
	decomposition (something like NFD, but not equivalent. Example: \u304C
	is completely equivalent to \u304B\u309B, which is not part of NFKD).
	This means, if there is a combined Kana characters, it will be first 
	decomposed and then compared.

*** Microsoft design problem

	Microsoft implementation seems to have a serious problem that many,
	many characters that are used in for each specific culture, such as
	Myanmar, Mongolian, Cherokee, Etiopic, Tagalog, Khmer, are regarded as
	"completely ignorable".


** MS collation design inference

*** Levels

	Each character has several "weights". It is a common concept between
	Windows and UCA.

	There are 5 levels:

	- level 1: primary difference
	  The first byte of level 1 means the category of the character.
	- level 2: diacritic difference, including Japanese voice mark countup
	- level 3: case/width sensitivity, and Hangul properties
	- level 4: kana weight (all of them have primary category 22, at least
	  in InvariantCulture)
	- level 5: variable weighting (apostrophe, hyphens etc.)

	Note that these levels does not digitally match IgnoreXXX flags. Thus
	it is not OK that we omit some levels of sortkey values in reflection
	to CompareOptions.

	String comparison is done from level 1 to 5. The comparison won't
	stop until either it found the primary difference, or it reached to
	the end (thus upper level differences are returned).

	For example, "e" is smaller than "E", but "eB" is bigger than "EA".
	If the collator just returned case difference at first 'e' and 'E',
	"eB" is still smaller than "EA".

**** level 5: variable weighting by StringSort

	There are some characters that are treated specially. Namely they are
	apostrophe and hyphens. The sortkeys for them is put after level 4
	(thus here I write them as "level 5"). It has different sort key
	format. See immediate below. There is no level 5 characters when 
	StringSort is specified.

*** sort key format

	00 means the end of sort key.
	01 means the end of the level.
	02-FF means the value.
	Actually '2' could be cut when all the following values are
	also '2' (i.e. the sort key binary won't contain extraneous '2').

	Every level has different key layout.

**** level 2

	For Japanese voice mark, it just sums the count up.

	For other letters, maybe there is a table.

**** level 3

	The actual values are + 2 (e.g. for Hangul Normal Jamo, the value is 4)

	For Korean letters:
		- 2: Normal Jamo (1100-11F9)
		- 4: Half width? (FFA0-FFDC) and Compatibility Jamo? (3165-318E)
		- 5: Compatibility Jamo (3130-3164)?
		     TODO: Learn about Korean characters.

	For numbers:
		- 4 circled inverse (2776-277F)
		- 8 circled sans serif (2780-2789)
		- C circled inverse && sans serif (278A-2793)
		- 18 Roman, 1BC (TODO: why?)

	For Arabic letters:
		- 2 Isolated form in presentation form B in FE80-FE8D
		- 4 Alef/Bet/Gimel/Dalet (2135-2138)
		- 8 Final form in presentation form B in FE82-FEF2
		- 18 Medial form in presentation form B in FE8C-FEF4

		Grep "ISOLATED", "FINAL" or "MEDIAL" against UnicodeData.txt
		(and filter by codepoints).
		or alternatively, see DerivedDecompositionType.txt.

		- 22 6A9 (TODO: what is it?)
		- 28 6AA (TODO: what is it?)

	For other letters:
		- 1 Fullwidth. UnicodeData.txt has <full>.
		- 2 Subscript. UnicodeData.txt has <sub>.
		- 8 Small capital, 03C2 (TODO: why?),
		    2104, 212B(flag=1A) (TODO: why?)
		    grep "SMALL CAPITAL" against UnicodeData.txt.
		- C only FE42. TODO: what is this?
		- E Superscripts. UnicodeData.txt has <super>.
		- 10 Uppercase.
		     DerivedCoreProperties.txt has Uppercase property.

	Note that simple 02 (value is 00) could be omitted.

	Summary: at least 7 bits are required as to represent a table -
	smallCapital, uppercase, normalization forms (2 bits:full/sub/super),
	arabic forms (2 bits:isolated/medial/final)

**** level 4

	Those sortkey data is collected only for Japanese (category 22)
	characters.

	There are 3 sections each of them ends by FF. Each of them represents
	the values for character by character:
	- small letter type (kogaki moji); C4 (small) or E4 (normal)
	- category middle section:
		+ dash mark 4 (\u309D,\u309E,\u30FD,\u30FE,\uFF70), 5 (\u30FC)
		  LAMESPEC: those characters of value '4' differs in level 2
		  wrt voice marks, but does not differetiate kana types (bug).
		  It is ignored when IgnoreNonSpace applies.
		'0x02'
		+ kana type: E4 (hiragana) or C4 (katakana)
	- width; C5 (full) or C4 (half)

**** level 5

	[offsetL]* [offsetM + 0x7F]? [3 + offsetS * 4] [6] [char-weight]

	where all "offsetL" "offsetM" and "offsetS" represents the offset
	from the previous level 5 character in the resulting string.
	"offsetL" continues until the value is 0xFF.
	"offsetM" is always larger than 0x80.

	The actual offset is 8064 * numOffsetL + 63 * offsetM + offsetS

	('3' may actually vary and/or represent another values but no idea.
	At least 00, 01 and 02 are not acceptable since they are reserved.
	02 is not reserved by definition above, but the key-size optimizer
	uses it as a special mark, as mentioned above.)

*** sort key table

	Here is the simple sortkey dumper:

	public static void Main (string [] args)
	{
		CultureInfo culture = args.Length > 0 ?
			new CultureInfo (args [0]) :
			CultureInfo.InvariantCulture;
		CompareInfo ci = culture.CompareInfo;
		for (int i = 0; i < char.MaxValue; i++) {
			string s = new string ((char) i, 1);
			if (ci.Compare (s, "") == 0)
				continue; // ignored
			byte [] data = ci.GetSortKey (s).KeyData;
			foreach (byte b in data) {
				Console.Write ("{0:X02}", b);
				Console.Write (' ');
			}
			Console.WriteLine (" : {0:X}, {1} {2}",
				i,
				Char.GetUnicodeCategory ((char) i),
				data [2] != 1 ? '!' : ' ');
		}
	}

*** multiple character mappings

	Some sequence of characters are considered as a "composite" that is
	to be composed either as another character or another sequence of 
	characters. Those "composite" might not have corresponding equivalent
	character in sortkey.
	Similarly, some single characters are expanded to a sequence of
	characters.

	The correspoinding implementation will be "CharacterIterator".

**** diacritic characters

	Except for those variable-weighting characters, there are only
	diacritical (or other kinds of nonspacing) characters that don't
	have primary weights.

	Diacritics are not regarded as a base character when placed after 
	(maybe some kind of) letters.

	The behavior is diacritic character dependent. For example, Japanese
	combination of a Kana character and a voice mark is compulsory (the
	resulting sort key is regarded as identical to the corresponding
	single character. Try \u304B\u309B with \u304C. It is invariant).

	In French cultures, diacritic orderings are checked from right to left.

**** Composite character processing

	There are some sequences of characters that are treated as another
	character or another sequence of characters.

	By default, there is no composite form.
	http://www.microsoft.com/globaldev/dis_v1/disv1.asp?DID=dis33d&File=S24C2.asp
	(Note that composite is different from expansion.)

	<del>
	According to "Developing International Software" book, in Win32
	lstrcmpi(), "sc" in hu-HU is treated as a single character, so if it is
	compared against "Sc" (in IgnoreCase), "sc" won't match it. However
	.NET (LCMapString) behaves differently.
	</del>
	It is wrong. It is "cs" that is treated specially, and also applies to
	LCMapString collation.

	Note that composite characters is likely to not have equivalent
	codepoint.

**** Expanded character processing

	Some characters are expanded to two or more characters:

	C6 (AE), E6 (ae), 1F1-1F3 (dz), 1C4-1C6 (Dz), FB00-FB06 (ff, fi),
	132-133 (IJ), 1C7-1C9 (LJ), 1CA-1CC (NJ), 152-153 (OE),
	DF (ss), FB06 (st), FB05 (\u017Ft), FE, DE, 5F0-5F2,
	1113-115F (hangul)
	(CJK extension is not really expanded)

	They don't match with any of Unicode normalization.

	Some alphabetic cultures have different mappings, but mostly small
	(at least da-DK, lt-LT, fr-FR, es-ES have tiny differences).

	Invariant culture also puts Czech unique character \u0161 between s
	and t, unlike described here:
	http://www.microsoft.com/globaldev/dis_v1/disv1.asp?DID=dis33d&File=S24C0.asp

*** default sort key table

**** StringSort

	When CompareOptions.StringSort is specified, then it modifies
	characters in category 2 from "1 1 1 1 80 07 06 xx" to
	"06 xx yy zz" and some characters become case sensitive.

	For details, "level 5" description above.

	To handle them simply, we introduce "category 0x01" (which never
	happens in the actual sortkeys) for those variables in the table.

	There seems no further differences between StringSort and None.

**** character category details

	1 specially ignored ones (Japanese, Tamil, Thai)

		Unicode: 3099-309C, BCD, E47, E4C, FF9E, FF9F
		SortKey: 01 01 01 01 00

	2 variable weight characters
	
	They are either at 01 01 01 01 or 06, depending on StringSort. For
	convenience, I use 06 to describe them.

	2.1 control characters (specified as such in Unicode), except for
	whitespaces (0009-000D).

		Unicode: 0001-000F minus 0009-000D, 007F-009F
		SortKey: 06 80 07 06 03 00 - 06 80 07 06 3D 00

	2.2 Apostrophe
		Unicode: 0027,FF07 (')
		SortKey: 06 80 (and nonspace equivalent)

	2.3  minus sign, hyphen, dash
	  minus signs: FE63, 207B (super), 208B (sub), 002D, 00FD (full-width)
	  hyphens: 00AD (soft), 2010, 2011 (nonbreaking) ... Unicode HYPHEN?
	  dashes, horizontal bars: FE58 ... UnicodeCategory.DashPunctuation

		SortKey: 06 81 - 06 90 (and nonspace equivalents)

	2.4 Arabic spacing and equivalents (64B-651, FE70-FE7F)
	  They are part of nonspacing mark, but not equal.

		SortKey: 06 A0 - 06 A7 (and nonspace equivalents)

	3 nonprimary characters, mixed.

	  ModifierSymbol, except for that are not in category 0 and "07" area
	  (i.e. < 128) nor those equivalents

	  NonSpacingMark which is ignorable (IsIgnorableNonSpacing())
	  // 30D, CD5-CD6, ABD, 2B9-2C1, 2C8, 2CB-2CD, 591-5C2. NonSpacingMark in
	  // 981-A3C. A4D, A70, A71, ABC ...

	  TODO: I need more insight to write table generator.

	  SortKey: 01 03 01 - 01 B6 01

	  This part of MS table design is problematic (buggy): \u0592 should
	  not be equal to \u09BC.

	  I guess, this buggy design is because Microsoft first thought that
	  there won't be more than 255 characters in this area. Or they might be
	  aware of the problem but prefer table optimization.

	  Ideal solutions:

	  1) We should not mix those code (make things sequential) and expands
	     level 2 length to 2 bytes. Instead of having direct value, we
	     could use index (pointer) to zero-terminating level 2 table.

	  2) Include those charactors from minor cultures here.

	  If in "discriminatory mode", those tables could be still provided
	  as to be compatible to Windows.

	4 space separators and some kind of marks

	4.1 whitespaces, paragraph separator etc.
	  UnicodeCategory.SpaceSeparator : 20, 3000, A0, 9-D, 2000-200B

	  SortKey : 07 02 - 07 18

	4.2 some OtherSymbols: 2422-2423
	
	  SortKey : 07 19 - 07 1A

	4.3 other marks ('!', '^', ...)
	  Non-alpha-numeric < 0x7F except for '+' (math) and '-' (math/hyphen)
	  some Punctuations: InitialQuote/FinalQuote/Open/Close/Connector
	  some OtherSymbols: 2400-2424
	  3003, 3006, 2D0, 10FB
	  remaining Puncuations: 9xx, 7xx
	  70F (Format)

	  SortKey : 07 1B - 07 F0

	5 mathmatical symbols
	  InitialQuotePunctuation and FinalQuotePunctuation in ASCII
	  (not Quotation_Mark property in PropList.txt ; 22, 27)

	  byte area MathSymbol: 2B,3C,3D,3E,AB,B1,BB,D7,F7 except for AC
	  MathSymbol (2044, 208A, 208C, 207A, 207C)
	  OtherLetter (1C0-1C2)
	  2200-22FF MathSymbol except for 221E (INF. ; regarded as a number)

	  SortKey : 08 02 - 08 F8

	6 Arrows and Box drawings
	  09 02 .. 09 7C : 2300-237A
	  09 BC ... 09 FE : 25A0-AB, 25E7-EB, 25AC-B5, 25EC-EF, 25B6-B9,
			   25BC-C3, 25BA-25BB, 25C4-25D8, 25E6, 25DA-25E5
			   21*,25*,26*,27*
	  2190- (non-codepoint order)
		note that there are many compatibility equivalents
	  2500- except for 266F (#)

	  SortKey : 09 02 - 09 7C, 09 BC 01 03 - 09 BC 01 13,
		    09 {BD|BE|BF} 01 {03|04}, ...
		    TODO: fill the patterns

	7 currency sumbols and some punctuations
	  byte CurrencySymbols except for 24 ($)
	  byte OtherSymbols (A7-B6) 
	  ConnectorPunctuation - 2040 (i.e. FF65, 30FB)
	  OtherPunct/ConnectorPunct/CurrencyCymbol 2020-20AC - 20AC
	  OtherSymbol 3012-303F,3004,327F
	  MathSymbol/OtherSymbol 2600-2767 (math = 266F)
	  OtherSymbol 2440-244A, 2117
	  20AC (CurrencySymbol)

	  Sortey : 0A 02 - 0A FB

	8 (C) numbers
	  all DecimalDigitNumber, LetterNumber, non-CJK OtherNumber.
	  9F8.
	  digits, in numeric order. We can use NET_2_0 CharUnicodeInfo.
	  221E. (INF.)

	  SortKey : 0C 02 (9F8), 0C 03 - 0C E1 (normal numbers), 0C FF (INF.)

	9 (E) latin letters (alphabets), mixing alphabetical symbols
	  Alphabets, A to Z, mixing alphabetical symbols.
	  F8-2B8 except for (1BB-1BD and 1C0-1C3), but not sequential.
	  2E0-2E3.

	  TODO: fill diacritical orders.

	  For 'A' it is "0E 02", for 'B' "0E 09" ... 'Z' "0E A9", ezh "0E AA".
	  0E B3 (1BE), 0E B4 (298)

	  This ordering is nothing to do with European Ordering Rules (EOR).

	10 (F) greek letters
	  0F: 386-3F2
	  10: 400-4E9 exc. 482-486
	  11: 531-586 exc. 559-55F
	  12: 5D0-5F2
	  13: 621-64A, 670-6D3, 6D5
	  14: 901-963 exc. 93C-93D 950-954
	  15: 982-9FA exc. NonSpacingMark DecimalDigitNumber OtherNumber
	  16: A05-A74 exc. A3C A4D A66-A71
	  17: A81-AE0 exc. ABC-ABD
	  18: 

	...

	   (21) georgian letters

	11 (22) japanese kana letters and symbols, not in codepoint order

	  For single character, the sortkeys look like:
	  - Katakana normal A, Half Width (FF71) : FF 02 C4 FF C4 FF 01 00
	  - Katakana normal A, Full Width (30A2) : FF C4 FF 01 00
	  - Hiragana normal A, Full Width (3042) : FF FF 01 00

	  Actually for level 4 weights, there is a different rule (see
	  "level 4" format above).

	  There is also 32D0 (normal katakana A with circle) that have
	  diacritic difference.

	  For primary weights, 'A' to 'O' are mapped to 22-26, 'Ka' to 'Ko'
	  are to 2A-2E, 'Sa' to 'So' are to 32-36 ... and follows.

	  After Kana characters, there are CJK compat characters.
	  From 22 97 01 01 01 01 00 (3349) to 22 A6 01 01 01 01 00 (333B) are
	  sorted in JIS table order (CP932.TXT). Others are unknown, but I
	  don't think the order really matters.

	  UCA DUCET also does not apply here.

	12 (23) bopomofo letters

	13 (24) syriac/thaana letters
	  710-72C exc. 711, 780-7A5.

	  Maybe we should add remaining minor-culture characters here.

	14 (41-45) surrogate Pt.1

	15 (52 02-7E C8) hangul, mixing combined ones

	  It starts from 1100. After width-insensitive equivalents, those
	  syllables (from AC00) follow (until AE4B). It follows kinda based
	  on some formula (sometimes it looks not e.g. 1117).

	16 (9E 02-F1 E4) CJK (kangxi etc.)

	   4E00-. Ordered, condidering case/width equivalents.

	17 (E5 02-FE 33) PrivateUse.

	   In fact it overlaps to CJK characters (maybe layout design failure).

	18 (F2 01-F2 31) surrogate Pt.2

	   In fact it overlaps to PrivateUse (maybe layout design failure).

	19 (FE FF 10 02 - FE FF 29 E9) CJK extensions

	   3400-4DB5. Ordered, considering case/width equivalents.

	20 (FF FF 01 01 01 01 00) Some supplemental Japanese/Arabic marks

	   3005, 3031, 3032, 309D, 309E, 30FC, 30FD, 30FE, FE7C, FE7D, FF70

	- by UnicodeCategory -

	DashPunctuation		1 1 1 1 (no exception)
	DecimalDigitNumber	C (no exception)
	EnclosingMark		1 E (no exception)
	Format			7 (only 70F)
	LetterNumber		C (no exception)
	LineSeparator		7 (only 2028)
	ParagraphSeparator	7 (only 2029)
	PrivateUse
	SpaceSeparator		7 (no exception)
	Surrogate

	OtherNumber		C(<3192), 9E-A7 (3124<)

	Control			1 1 1 1 except for 9-D (7)
	FinalQuotePunctuation	7 except for BB (8)
	InitialQuotePunctuation	7 except for AB (8)
	ClosePunctuation	7 except for 232A (9)
	OpenPunctuation		7 except for 2329 (9)
	ConnectorPunctuation	7 except for FF65, 30FB, 2040 (A)

	OtherLetter		1, 7, 8 (1C0-1C2), C, 12-FF
	MathSymbol		8, 9, 1 1 1 1, 7, A, C
	OtherSymbol		7, 9, A, C, E, F, <22, 52<
	CurrencySymbol		A except for FF69,24,FF04 (7) and 9F2,9F3 (15)

	LowercaseLetter		E-11 except for B5 (A) and 1BD (C)
	TitlecaseLetter		E (no exception)
	UppercaseLetter		E,F,10,11,21 except for 1BC (C)
	ModifierLetter		1, 7, E, 1F, FF
	ModifierSymbol		1 1 1 1, 1, 7
	NonSpacingMark		1 1 1 1, 1, 13-1F
	OtherPunctuation	1, 7, A, 1F
	SpacingCombiningMark	1, 14-22

*** Culture dependent design

	(To assure this section, run the simple dumper code shown above,
	with all the supported cultures.)

**** primary cultures and non-primary cultures

	This code is used to iterate character dump through all cultures,
	using sort key dumper put above.

	public static void Main ()
	{
		foreach (CultureInfo ci in CultureInfo.GetCultures (
			CultureTypes.AllCultures)) {
			ProcessStartInfo psi = new ProcessStartInfo ();
			psi.FileName = "../allsortkey.exe";
			psi.Arguments = ci.Name;
			psi.RedirectStandardOutput = true;
			psi.UseShellExecute = false;
			Process p = new Process ();
			p.StartInfo = psi;
			p.Start ();
			string s = p.StandardOutput.ReadToEnd ();
			StreamWriter sw = new StreamWriter (ci.Name + ".txt", false, Encoding.UTF8);
			sw.Write (s);
			sw.Close ();
		}
	}

	For each sub culture (that has a parent culture), its collation
	mapping is identical to that of its parent, except for az-AZ-Cyrl.

	Additionally,

	- zh-CHS = zh-CN = zh-SG = zh-MO : pronounciation
	- zh-TW = zh-HK = zh-CHT : stroke count
	- da = no
	- fi = sv
	- hr = sr

	(UCA implies that there are some cultures that sorts alphabets from
	large to small, but as long as I see there is no such CultureInfo.)

**** Latin characters and NonSpacingMark order tailorings

	div : FDF2 is 24 83 01 01 01 01 00 (only 1 difference)
	syr : some NonSpacingMarks are totally ignorable.
	tt,kk,mk,az-AZ-Cyrl,uk : cyrillic difference
	az,et,lt,lv,sl,tr,sv,ro,pl,no,is,hu,fi,es,da : latin difference
	fr : 1C4-1C6.
	sk,hr,cs : latin and NonSpacingMark differences

	ja,ko : 5C

**** CJK character order tailorings

	<how many tables?>

	There are five different CJK orderings:
	default, ko(-KR), ja(-JP), zh-CHS and zh-TW
	They have very different CJK mapping for each.

	Since they are mostly computational differences, we are not likely to
	extend those character weights into constant tables unless they are
	required (actually for Japanese it is partly required).

	<what characters are variable?>

	ko : CJK layout difference (52 -> 80)
	ja,zh-CHS,zh-TW : dash (5C), CJK layout difference.

	Target characters are : CJK misc (3190-), Parenthesized CJK
	(3200-), CJK compat (3300-), CJK ideographs (4E00-),
	CJK compat ideograph (F900-), Half/Full width compat (FF00-)

	Additionally for Korean: Jamo (1100-), Hangle syllables (AC00)

	<how do they consist of?>

	Japanese CJK order looks based on JIS table order. Those characters
	which are also in JIS table are moved to 80 xx. Those which are *not*
	in JIS table are left as is (9E-FE).

	Korean CJK order looks similar that respects KS C 5619.

	For some Chinese such as zh-CHS, character order is based on pinyin.

	And for remaining Chinese such as zh-TW, it is stroke count based.

	CLDR of unicode.org has reference ordering of those characters, so
	I am going to extract the sorting order data from it:
	http://www.unicode.org/cldr/

**** Accent evaluation order

	With French cultures, diacritical marks must be put *in front of the
	character being decorated (aka French ordering).
	French ordering does not affect only on some diacritics (Japanese
	voice mark is not affected).

	Some other cultures might also have different ones, but not obvious.


** Mono implementation plans

*** CultureInfoImpl

	CultureInfo contains many overloaded methods that are just for 
	convenience. This class contains only required members.

*** MSCompatUnicodeTable

	Provides several character information:

	- ignorable, ignorable nonspace, normalize width, normalize kanatype
	- level 4 sortkey provision method(s)
	- bool HasLevel3(char)
	- bool HasVariableWeight(char)

	- bool HasContractionAsHead(char)
	- bool HasContractionAsTail(char)
	- int IndexToContraction(char)
	- char [] ContractionTable
	- bool HasExpansion(char)
	- int IndexToExpansion(char)
	- char [] ExpansionTable

	- default sortkey table
	- culture dependent CJK table
	- culture dependent sortkey adjustment table
	  (culture-specific modification)

*** CharacterIterator

	The match evaluation could not be done char by char - the longest
	possible sequence of characters in the tailored table (e.g. "ch" 
	in Spanish) should be examined.

	(Some examples can be seen in UTR#10).

	It should work like TextElementEnumerator but should not result in a
	bunch of string instances. The simple set of below is enough:

	- MoveNext(),
	- MoveBack(),
	- Current,
	- ElementLength, and
	- Reset()

	They could be implemented as an internal virtual method of CompareInfo.

	This resolves combined characters and expanded characters, including
	French accent orderings. The iteration logic will be, however, only
	one, and it will use culture-dependent character combination/expansion
	tables.

**** character comparison

	Since composite character is likely to *not have* equivalent
	codepoint, character comparison could not just be done by expecting
	"resulting char" value.
	In contrast, since composite character is likely to *do have*
	equivalent codepoint, character comparison could not also just be done
	by comparing "source char" value.

	Collator.CompareChar() compares character elements in two character 
	iterators and returns the difference "level". The returned value is
	used either to check equality or save difference level (see Compare()
	description for details).

	Character iterator comparison is first done by codepoints i.e. it
	first detects where the codepoints differ.

	From where those codepoints differ, for each iterators it adjusts the
	position so that it represents exactly one character element. That is,
	find primary character as the start of the range and the last
	nonprimary character as the end of the range.

	Once the comparer adjusted the character iterator to be valid
	comparison position, further comparison is done by following steps:

	Then, return comparison results of sortkeys of n(s, l, f), where
	function n returns keys for such s, l, f that:
	- s : the source character (or string) of character iterator,
	- l : the comparison level
	- f : CompareOption (note that it is different from level)

*** sort key element table

	We will create *our own* collation element table which is closer
	to the one from Windows than UCA default element table, but should
	fix their bugs such as ignoring minor culture. We might provide
	"discriminatory mode" that behaves closer to Windows (that ignores
	some minor cultures).

**** Characters in the table / characters computed

	Currently I plan not to contain following characters in the table
	but compute on demand:

	- PrivateUse
	- Surrogate
	- Hangul Syllables

**** CJK Unified Ideographs

	For CJK unified ideographs, I have to make those culture-dependent
	tables in memory. They will be in separate table.
	Since they came from some classical encodings, they are not computed.

	Culture-dependent rules are always "evaluated", except for radical
	character mapping differences (i.e. ja, kr, zh-*). Other than that,
	no physical expansion is done to the table loaded in memory.
	(It's waste of memory.)

**** Level 4: Kana type

	The table is not likely to contain level 4 (kanatype) properties for
	the whole characters; It will be provided only for Japanese characters
	and will be single byte information (flags).

**** Level 3: Case properties

	Case properties will be stored as a byte array, with limited areas of
	codepoint (cp < 3120 || FE00 < cp).

	For Hangul characters, it will be computed by codepoint areas.

**** Level 2: Diacritical properties

	The table will be composed as two bytes for a character. Actually when
	MS compatible (discriminatory) mode, the second byte will be always 0.

	Note that Japanese voice marks are considered at level 2 but no need to
	have maps.


** Reference materials

	Developing International Software for Windows 95 and Windows NT
	Appendix D Sort Order for Selected Languages
	http://www.microsoft.com/globaldev/dis_v1/disv1.asp?DID=dis33d&File=S24BF.asp

	UTR#10 Unicode Collation Algorithm (It is still informative)
	http://www.unicode.org/reports/tr10/

	UAX#15 Unicode Normalization
	http://www.unicode.org/reports/tr15/
	especially its canonical/compatibility equivalent characters might
	be informative to get those equivalent characters.
	
	To know which character can be expanded, Unicode Character Database
	(UCD) is informative (it's informative but not normative to us)
	http://www.unicode.org/Public/UNIDATA/UCD.html

	Wine uses UCA default element table, but has windows-like character
	filterings support in their LCMapString implementation:
	http://cvs.winehq.com/cvsweb/wine/dlls/kernel/locale.c
	http://cvs.winehq.com/cvsweb/wine/libs/unicode/sortkey.c

	Mimer has decent materials on culture specific collations:
	http://developer.mimer.com/collations/
